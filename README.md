# Scraping Google Flights - Projet AutomatisÃ©

## ğŸ“‹ Description

Ce projet permet de scraper automatiquement les prix de vols sur Google Flights pour analyser les tendances de prix sur plusieurs destinations. Le systÃ¨me fonctionne 24h/24 sur un Raspberry Pi et transfÃ¨re automatiquement les donnÃ©es vers votre ordinateur principal.

## ğŸ¯ FonctionnalitÃ©s

- **Scraping automatique** : Collecte des donnÃ©es 3 fois par jour (8h, 14h, 20h)
- **Multiples destinations** : 16 destinations prÃ©configurÃ©es (Europe, AmÃ©rique, Asie, Afrique)
- **DonnÃ©es complÃ¨tes** : Prix, horaires, durÃ©e, escales, compagnies
- **Stockage CSV** : Sauvegarde automatique en format CSV
- **Transfert SFTP** : Envoi automatique vers votre ordinateur
- **Logs dÃ©taillÃ©s** : Suivi complet des opÃ©rations
- **Maintenance automatique** : Nettoyage des logs et mise Ã  jour des dates

## ğŸ—ï¸ Architecture

```
scrapFlights/
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ scraper.py           # Module de scraping
â”œâ”€â”€ scheduler.py         # Planification automatique
â”œâ”€â”€ sftp_transfer.py     # Transfert SFTP
â”œâ”€â”€ config.py            # Configuration
â”œâ”€â”€ requirements.txt     # DÃ©pendances
â””â”€â”€ donnees/            # Dossier des donnÃ©es
    â”œâ”€â”€ vols_data.csv   # DonnÃ©es collectÃ©es
    â”œâ”€â”€ scraping.log    # Logs de scraping
    â”œâ”€â”€ scheduler.log   # Logs de planification
    â””â”€â”€ sftp_transfer.log # Logs de transfert
```

## ğŸš€ Installation

### 1. PrÃ©requis sur Raspberry Pi (Debian)

```bash
# Mise Ã  jour du systÃ¨me
sudo apt update && sudo apt upgrade -y

# Installation de Python et pip
sudo apt install python3 python3-pip python3-venv -y

# Installation de Chrome (nÃ©cessaire pour Selenium)
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
sudo apt update
sudo apt install google-chrome-stable -y

# Installation des dÃ©pendances systÃ¨me
sudo apt install chromium-chromedriver -y
```

### 2. Configuration du projet

```bash
# Cloner ou tÃ©lÃ©charger le projet
cd /home/pi/
git clone <votre-repo> scrapFlights
cd scrapFlights

# CrÃ©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate

# Installer les dÃ©pendances Python
pip install -r requirements.txt
```

### 3. Configuration

Ã‰ditez le fichier `config.py` pour personnaliser :

```python
# Modifier les destinations selon vos besoins
DESTINATIONS = [
    {"depart": "Paris", "arrivee": "Londres", "code_depart": "CDG", "code_arrivee": "LHR"},
    # Ajouter vos destinations...
]

# Configuration SFTP (votre ordinateur)
SFTP_CONFIG = {
    "hostname": "192.168.1.100",  # IP de votre ordinateur
    "username": "votre_username",
    "password": "votre_password",
    "port": 22,
    "remote_path": "/home/user/flights_data"
}
```

## ğŸ“Š Utilisation

### Test initial

```bash
# Test d'une seule session de scraping
python main.py --once
```

### DÃ©marrage du systÃ¨me complet

```bash
# DÃ©marrer le scheduler automatique
python main.py --scheduler
```

### ArrÃªt du systÃ¨me

```bash
# Utiliser Ctrl+C pour arrÃªter proprement
```

## ğŸ”§ Configuration SFTP sur votre ordinateur

### Windows

1. **Installer un serveur SFTP** (WinSCP, FileZilla Server)
2. **Ou utiliser WSL** avec OpenSSH :

```bash
# Dans WSL
sudo apt install openssh-server
sudo systemctl start ssh
sudo systemctl enable ssh
```

### Linux/Mac

```bash
# Installer OpenSSH
sudo apt install openssh-server  # Ubuntu/Debian
sudo systemctl start ssh
sudo systemctl enable ssh
```

## ğŸ“ˆ Analyse des donnÃ©es

Les donnÃ©es sont sauvegardÃ©es dans `vols_data.csv` avec les colonnes :

- `date_collecte` : Date et heure de la collecte
- `depart` : Ville de dÃ©part
- `arrivee` : Ville d'arrivÃ©e
- `date_depart` : Date du vol
- `prix` : Prix en euros
- `compagnie` : Compagnie aÃ©rienne
- `heure_depart` : Heure de dÃ©part
- `heure_arrivee` : Heure d'arrivÃ©e
- `duree_vol` : DurÃ©e du vol
- `escales` : Nombre d'escales
- `url_source` : URL source

## ğŸ” Surveillance et logs

### Logs disponibles

- `donnees/scraping.log` : Logs du scraping
- `donnees/scheduler.log` : Logs de planification
- `donnees/sftp_transfer.log` : Logs de transfert
- `donnees/main.log` : Logs principaux

### Surveillance en temps rÃ©el

```bash
# Suivre les logs en temps rÃ©el
tail -f donnees/main.log

# VÃ©rifier l'Ã©tat du systÃ¨me
ps aux | grep python
```

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨mes courants

1. **Chrome ne dÃ©marre pas** :
   ```bash
   sudo apt install --reinstall google-chrome-stable
   ```

2. **Erreur de connexion SFTP** :
   - VÃ©rifier l'IP et les credentials dans `config.py`
   - Tester la connexion SSH manuellement

3. **Pas de donnÃ©es rÃ©cupÃ©rÃ©es** :
   - VÃ©rifier la connexion internet
   - Consulter les logs pour les erreurs
   - Google peut bloquer temporairement

### Maintenance

```bash
# Nettoyer les anciens logs
find donnees/ -name "*.log" -mtime +30 -delete

# VÃ©rifier l'espace disque
df -h

# RedÃ©marrer le service
pkill -f main.py
python main.py --scheduler
```

## ğŸ“Š Exemple d'analyse des donnÃ©es

```python
import pandas as pd
import matplotlib.pyplot as plt

# Charger les donnÃ©es
df = pd.read_csv('donnees/vols_data.csv')

# Analyser les prix moyens par destination
prix_moyens = df.groupby('arrivee')['prix'].mean().sort_values(ascending=False)
print(prix_moyens)

# Tendance des prix dans le temps
df['date_collecte'] = pd.to_datetime(df['date_collecte'])
prix_temps = df.groupby(df['date_collecte'].dt.date)['prix'].mean()
prix_temps.plot()
plt.show()
```

## ğŸ”’ SÃ©curitÃ©

- Utilisez des mots de passe forts pour SFTP
- ConsidÃ©rez l'utilisation de clÃ©s SSH
- Limitez l'accÃ¨s rÃ©seau au Raspberry Pi
- Surveillez rÃ©guliÃ¨rement les logs

## ğŸ“ Support

En cas de problÃ¨me :
1. Consultez les logs dans le dossier `donnees/`
2. VÃ©rifiez la configuration dans `config.py`
3. Testez avec `python main.py --once` pour isoler le problÃ¨me

## ğŸ“ Notes importantes

- Le scraping respecte les dÃ©lais entre les requÃªtes
- Les donnÃ©es sont sauvegardÃ©es automatiquement
- Le systÃ¨me redÃ©marre automatiquement en cas d'erreur
- Surveillez l'espace disque sur le Raspberry Pi 